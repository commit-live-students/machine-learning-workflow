{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# The story so far\n",
    "\n",
    "![](../images/ml_workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaluation Metrics:\n",
    "\n",
    "* Machine learnig algorithms work on feedbacks.\n",
    "* Evaluation metrics are used to decide how they performed on a given dataset with given hyper-parameters.\n",
    "* Evalutaion metrics should always be tied with the *business objectives*.\n",
    "\n",
    "## Example\n",
    "\n",
    "For example,\n",
    "\n",
    "* We are trying to detect credit card fraud.\n",
    "* Occurance rate of fraud is 4 in 1000.\n",
    "* Let's say our model predicted as following \n",
    "\n",
    "| |Farud|Not Fraud|\n",
    "|---|---|---|\n",
    "|Predicted Fraud|2|1|\n",
    "|Predicted not Fraud|1|996|\n",
    "\n",
    "* If we calculate the accuracy of the model, it will 99.8%. However, that is not the accurate representation of the performance of our model.\n",
    "\n",
    "## Choosing a Metric\n",
    "\n",
    "Let us try to design an evaluation metric that is more appropriate for our problem.\n",
    "\n",
    "* It is really crucial to identify the fraud case correctly, hence every fraud case identified as *not fraud* should be heavily penalised.\n",
    "* Predicting a genuin case as fraud is although not acceptable, does not have that severe impact on the businss\n",
    "\n",
    "\n",
    "* Hence, let's assign penalty of 5 for every fraud classified as not fraud\n",
    "* and penalty of 1 for every genuin case identified as fraud.\n",
    "\n",
    "* Hence, our new metric could be \n",
    "\n",
    "> metric = (5 * false negative + 1 * false positive) / 6\n",
    "\n",
    "* Let's look some of the metrics that are used in real life machine learning problems.\n",
    "\n",
    "## Classification Metrics\n",
    "\n",
    "1. Classification Accuracy.\n",
    "* Logarithmic Loss.\n",
    "* Area Under ROC Curve.\n",
    "* Confusion Matrix.\n",
    "* Classification Report.\n",
    "\n",
    "## Classification Accuracy\n",
    "\n",
    "Classification accuracy is **the number of correct predictions made as a ratio of all predictions made.**\n",
    "\n",
    "* The most common evaluation metric for classification problems\n",
    "* Suitable when \n",
    "    * There are an equal number of observations in each class \n",
    "    * That all predictions and prediction errors are equally important, \n",
    "\n",
    "which is often not the case.\n",
    "\n",
    "Below is an example of calculating classification accuracy.\n",
    "\n",
    "* sklearn returns a ratio. \n",
    "* This can be converted into a percentage by multiplying the value by 100.\n",
    "\n",
    "Let's see how to use accuracy with sklearn.\n",
    "\n",
    "`sklearn`'s metric module provides us easy api's to use these evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`DEMO 2.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = [0, 2, 1, 3]\n",
    "y_true = [0, 1, 2, 3]\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Example with a Dataframe\n",
    "\n",
    "Let's see a dataframe example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`DEMO 2.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# load the iris datasets\n",
    "dataset = datasets.load_iris()\n",
    "\n",
    "# fit a logistic regression model to the data\n",
    "model = LogisticRegression()\n",
    "model.fit(dataset.data, dataset.target)\n",
    "print(model)\n",
    "\n",
    "# make predictions\n",
    "expected = dataset.target\n",
    "predicted = model.predict(dataset.data)\n",
    "\n",
    "# summarize the fit of the model\n",
    "print(metrics.accuracy_score(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## With Cross-validation\n",
    "\n",
    "Quite often, we use cross-validation techniques, and use the evaluation metrics to evaluate each round's performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`DEMO 2.3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation Classification Accuracy\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "\n",
    "scoring = 'accuracy'\n",
    "\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(\"Accuracy: {:.3f} ({:.3f})\".format(results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Logarithmic Loss\n",
    "\n",
    "Logarithmic loss (or logloss) is a performance metric for evaluating the predictions of probabilities of membership to a given class.\n",
    "\n",
    "> Log Loss = $- \\frac{1}{N} \\sum_{i=1}^N [y_{i} \\log \\, p_{i} + (1 - y_{i}) \\log \\, (1 - p_{i})].$\n",
    "\n",
    "where \n",
    "\n",
    "* N is the number of samples or instances, \n",
    "* M is the number of possible labels, \n",
    "* $ y_{ij}$ is a binary indicator of whether or not label j is the correct classification for instance i,\n",
    "* $ p_{ij}$ is the model probability of assigning label j to instance i.\n",
    "\n",
    "\n",
    "* The scalar probability between 0 and 1 can be seen as a measure of confidence for a prediction by an algorithm.\n",
    "* Predictions that are correct or incorrect are rewarded or punished proportionally to the confidence of the prediction.\n",
    "\n",
    "An example of calculating logloss for Logistic regression predictions on the Pima Indians onset of diabetes dataset.\n",
    "\n",
    "* logloss nearer to 0 is better, with 0 representing a perfect logloss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`DEMO 2.4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "\n",
    "scoring = 'neg_log_loss'\n",
    "\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(\"Logloss: {:.3f} {:.3f}\".format(results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## F-1 score\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/7d63c1f5c659f95b5dfe5893213cc8ea7f8bea0a)\n",
    "\n",
    "Where\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/26106935459abe7c266f7b1ebfa2a824b334c807)\n",
    "\n",
    "\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/4c233366865312bc99c832d1475e152c5074891b)\n",
    "\n",
    "* tp = true positive\n",
    "- tn = true negative\n",
    "- fp = false positive\n",
    "- fn = false negative\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/440px-Precisionrecall.svg.png)\n",
    "source: https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/440px-Precisionrecall.svg.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "`DEMO 2.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "\n",
    "scoring = 'f1'\n",
    "\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(\"f1 score: {:.3f} {:.3f}\".format(results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Area Under ROC Curve\n",
    "\n",
    "* Area under ROC Curve (or AUC for short) is a performance metric for binary classification problems.\n",
    "\n",
    "\n",
    "* The AUC represents a model’s ability to discriminate between positive and negative classes. \n",
    "* An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random. \n",
    "\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/ROC_curves.svg/709px-ROC_curves.svg.png)\n",
    "[source](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/ROC_curves.svg/709px-ROC_curves.svg.png)\n",
    "\n",
    "* ROC can be broken down into sensitivity and specificity. \n",
    "* A binary classification problem is really a trade-off between sensitivity and specificity.\n",
    "\n",
    "\n",
    "* Sensitivity is the true positive rate also called the recall. \n",
    "* It is the number instances from the positive (first) class that actually predicted correctly.\n",
    "\n",
    "\n",
    "* Specificity is also called the true negative rate. \n",
    "* Is the number of instances from the negative class (second) class that were actually predicted correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`DEMO 2.6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "\n",
    "scoring = 'roc_auc'\n",
    "\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(\"AUC: {:.3f} ({:.3f})\".format(results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "* The confusion matrix is a handy presentation of the accuracy of a model with two or more classes.\n",
    "\n",
    "![](http://www.dataschool.io/content/images/2015/01/confusion_matrix_simple2.png)\n",
    "[source](http://www.dataschool.io/content/images/2015/01/confusion_matrix_simple2.png)\n",
    "\n",
    "* **true positives** (TP): These are predicted yes and actually yes\n",
    "* **true negatives** (TN): We predicted no, and actually no\n",
    "* **false positives** (FP): We predicted yes, but actually no. (Also known as a \"Type I error.\")\n",
    "* **false negatives** (FN): We predicted no, but yes. (Also known as a \"Type II error.\")\n",
    "\n",
    "\n",
    "\n",
    "* The table presents predictions on the x-axis and accuracy outcomes on the y-axis. \n",
    "* The cells of the table are the number of predictions made by a machine learning algorithm.\n",
    "\n",
    "True Negative and True Positive predictions fall on the diagonal line of the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`DEMO 2.7`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "matrix = confusion_matrix(Y_test, predicted)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Classification Report\n",
    "\n",
    "* Scikit-learn does provide a convenience report when working on classification problems to give you a quick idea of the accuracy of a model using a number of measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`DEMO 2.8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "report = classification_report(Y_test, predicted)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Regression Metrics\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this section will review 3 of the most common metrics for evaluating predictions on regression machine learning problems:\n",
    "\n",
    "1. Mean Absolute Error.\n",
    "2. Mean Squared Error.\n",
    "3. $R^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Mean Absolute Error\n",
    "\n",
    "* The Mean Absolute Error (or MAE) is the sum of the absolute differences between predictions and actual values. \n",
    "* It gives an idea of how wrong the predictions were.\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/3ef87b78a9af65e308cf4aa9acf6f203efbdeded)\n",
    "\n",
    "* The measure gives an idea of the magnitude of the error, but no idea of the direction (e.g. over or under predicting).\n",
    "\n",
    "A value of 0 indicates no error or perfect predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`DEMO 2.9`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\"\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "dataframe = pandas.read_csv(url, delim_whitespace=True, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "seed = 7\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = LinearRegression()\n",
    "\n",
    "scoring = 'neg_mean_absolute_error'\n",
    "\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(\"MAE: {:.3f} ({:.3f})\".format(results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Mean Squared Error\n",
    "\n",
    "* The Mean Squared Error (or MSE) is much like the mean absolute error in that it provides a gross idea of the magnitude of error.\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/67b9ac7353c6a2710e35180238efe54faf4d9c15)\n",
    "\n",
    "* Taking the square root of the mean squared error converts the units back to the original units of the output variable and can be meaningful for description and presentation. \n",
    "* This is called the Root Mean Squared Error (or RMSE).\n",
    "\n",
    "* This metric is inverted so that the results are increasing. \n",
    "* Remember to take the absolute value before taking the square root if you are interested in calculating the RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`DEMO 2.10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\"\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "dataframe = pandas.read_csv(url, delim_whitespace=True, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "seed = 7\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = LinearRegression()\n",
    "\n",
    "scoring = 'neg_mean_squared_error'\n",
    "\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(\"MSE: {:.3f} ({:.3f})\".format(results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## $R^2$ Metric\n",
    "\n",
    "* The $R^2$ (or R Squared) metric provides an indication of the goodness of fit of a set of predictions to the actual values. \n",
    "* In statistical literature, this measure is called the coefficient of determination.\n",
    "\n",
    "This is a value between 0 and 1 for no-fit and perfect fit respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`DEMO 2.11`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\"\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "dataframe = pandas.read_csv(url, delim_whitespace=True, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "seed = 7\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = LinearRegression()\n",
    "\n",
    "scoring = 'r2'\n",
    "\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(\"R sq: {:.3f} ({:.3f})\".format(results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "# Sampling for Machine Learning\n",
    "\n",
    "## Overview\n",
    "\n",
    "* Validation is a technique which involves reserving a particular sample of a data set on which you do not train the model.\n",
    "* Later, you test the model on this sample before finalizing the model (model validation)\n",
    "\n",
    "Methods for model validation:\n",
    "\n",
    "1. Holdout sets\n",
    "2. Cross-validation\n",
    "    - k-fold validation\n",
    "3. hold-one-out\n",
    "\n",
    "## Why we validate\n",
    "\n",
    "* Preventing information leakage\n",
    "* Measuring the model's ability to generalise\n",
    "\n",
    "## Validation through Holdout Set\n",
    "\n",
    "* keep a portion of train data separate for validation.\n",
    "\n",
    "`sklearn`'s `cross_validation` provides `train_test_split` api. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Example\n",
    "\n",
    "In the following example, the nearest-neighbor classifier is about 90% accurate on this hold-out set.\n",
    "\n",
    "The hold-out set is similar to unknown data, because the model has not \"seen\" it before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`DEMO 2.12`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# split the data with 50% in each set\n",
    "X1, X2, y1, y2 = train_test_split(X, y, random_state=0,\n",
    "                                  train_size=0.5)\n",
    "\n",
    "# fit the model on one set of data\n",
    "model.fit(X1, y1)\n",
    "\n",
    "# evaluate the model on the second set of data\n",
    "y2_model = model.predict(X2)\n",
    "accuracy_score(y2, y2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model validation via cross-validation\n",
    "\n",
    "* Disadvantage of using a holdout set for model validation is that we have lost a portion of our data to the model training.\n",
    "\n",
    "* This is not optimal, and can cause problems – especially if the initial set of training data is small.\n",
    "\n",
    "* One way to address this is to use *cross-validation*; do a sequence of fits where each subset of the data is used both as a training set and as a validation set.\n",
    "\n",
    "![](https://github.com/jakevdp/PythonDataScienceHandbook/raw/475499f1464bcdf96e618c922a8e6c92b190ee9a/notebooks/figures/05.03-2-fold-CV.png)\n",
    "[source](https://github.com/jakevdp/PythonDataScienceHandbook/raw/475499f1464bcdf96e618c922a8e6c92b190ee9a/notebooks/figures/05.03-2-fold-CV.png)\n",
    "\n",
    "Here we do two validation trials, alternately using each half of the data as a holdout set.\n",
    "Using the split data from before, we could implement it like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`DEMO 2.13`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y2_model = model.fit(X1, y1).predict(X2)\n",
    "y1_model = model.fit(X2, y2).predict(X1)\n",
    "accuracy_score(y1, y1_model), accuracy_score(y2, y2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Notes\n",
    "\n",
    "* Accuracy scores could be combined (by, say, taking the mean) to get a better measure of the global model performance.\n",
    "* This particular form of cross-validation is a *two-fold cross-validation*\n",
    "\n",
    "## Model validation through k-fold validation\n",
    "\n",
    "We could expand on this idea to use even more trials, and more folds in the data\n",
    "![](https://github.com/jakevdp/PythonDataScienceHandbook/raw/475499f1464bcdf96e618c922a8e6c92b190ee9a/notebooks/figures/05.03-5-fold-CV.png)\n",
    "[source](https://github.com/jakevdp/PythonDataScienceHandbook/raw/475499f1464bcdf96e618c922a8e6c92b190ee9a/notebooks/figures/05.03-5-fold-CV.png)\n",
    "\n",
    "* Here we split the data into five groups, and use each of them in turn to evaluate the model fit on the other 4/5 of the data.\n",
    "* We can use Scikit-Learn's ``cross_val_score`` convenience routine to do it succinctly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`DEMO 2.14`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model validation using Leave-one-out validation (optional)\n",
    "\n",
    "* In this approach, we reserve only one data-point of the available data set. \n",
    "* And, train model on the rest of data set. This process iterates for each data point.\n",
    "* We make use of all data points, hence low bias.\n",
    "* This approach leads to higher variation in testing model effectiveness because we test against one data point. \n",
    "* So, our estimation gets highly influenced by the data point. \n",
    "* If the data point turns out to be an outlier, it can lead to higher variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`DEMO 2.15`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import LeaveOneOut\n",
    "scores = cross_val_score(model, X, y, cv=LeaveOneOut(len(X)))\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Notes \n",
    "\n",
    "* Because we have 150 samples, the leave one out cross-validation yields scores for 150 trials, and the score indicates either successful (1.0) or unsuccessful (0.0) prediction.\n",
    "* Taking the mean of these gives an estimate of the error rate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`DEMO 2.16`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "# Training Models\n",
    "\n",
    "1. Get Data\n",
    "2. Choose a class of model\n",
    "3. Choose model hyperparameters\n",
    "4. Arrange data into a features matrix and target vector\n",
    "5. Fit the model to your data\n",
    "6. Predict labels for unknown data\n",
    "7. Assess Model\n",
    "\n",
    "              - Collecting Data\n",
    "              - Exploratory Data Visualization\n",
    "              - Data pre-processing\n",
    "              - Feature Extraction\n",
    "              - Sampling\n",
    "              - Model Training and Tuning\n",
    "              - Model Assessment\n",
    "              - Model Deployment\n",
    "              - Importance of having a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "## Parameters vs Hyperparameters\n",
    "\n",
    "* Hyperparameters are parameters whose values are set prior to the commencement of the learning process.\n",
    "* By contrast, the value of other parameters is derived via training.\n",
    "\n",
    "## What is Hyperparameter Tuning\n",
    "\n",
    "Hyperparameter optimization or model selection is the problem of choosing a set of optimal hyperparameters for a learning algorithm, usually with the goal of optimizing a measure of the algorithm's performance on an independent data set.\n",
    "\n",
    "## What is Hyperparameter Tuning about\n",
    "\n",
    "So, to summarize. Hyperparameters:\n",
    "\n",
    "* Define higher level concepts about the model such as complexity, or capacity to learn.\n",
    "* Cannot be learned directly from the data in the standard model training process and need to be predefined.\n",
    "* Can be decided by setting different values, training different models, and choosing the values that test better.\n",
    "\n",
    "## Some examples of hyperparameters\n",
    "\n",
    "* Number of leaves or depth of a tree\n",
    "* Number of latent factors in a matrix factorization\n",
    "* Learning rate (in many models)\n",
    "* Number of hidden layers in a deep neural network\n",
    "* Number of clusters in a k-means clustering\n",
    "\n",
    "## Overview of Methods\n",
    "\n",
    "* Grid Search\n",
    "* Random Search\n",
    "* Bayesian Search (we'll not cover this one)\n",
    "\n",
    "\n",
    "## Grid Search\n",
    "\n",
    "* Machine learning models are parameterized so that their behavior can be tuned for a given problem. \n",
    "* Models can have many parameters and finding the best combination of parameters can be treated as a search problem.\n",
    "\n",
    "* Grid search is an approach to parameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid.\n",
    "\n",
    "The following code evaluates different alpha values for the Ridge Regression algorithm on the diabetes dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`DEMO 2.17`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Grid Search for Algorithm Tuning\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# load the diabetes datasets\n",
    "dataset = datasets.load_diabetes()\n",
    "\n",
    "# prepare a range of alpha values to test\n",
    "alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "\n",
    "# create and fit a ridge regression model, testing each alpha\n",
    "model = Ridge()\n",
    "grid = GridSearchCV(estimator=model, param_grid=dict(alpha=alphas))\n",
    "grid.fit(dataset.data, dataset.target)\n",
    "\n",
    "print(grid)\n",
    "\n",
    "# summarize the results of the grid search\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Random Search\n",
    "\n",
    "* Random search is an approach to parameter tuning that will sample algorithm parameters from a random distribution (i.e. uniform) for a fixed number of iterations. \n",
    "* A model is constructed and evaluated for each combination of parameters chosen. \n",
    " \n",
    "The following code evaluates different alpha random values between 0 and 1 for the Ridge Regression algorithm on the diabetes dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`DEMO 2.18`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Randomized Search for Algorithm Tuning\n",
    "import numpy as np\n",
    "from scipy.stats import uniform as sp_rand\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "# load the diabetes datasets\n",
    "dataset = datasets.load_diabetes()\n",
    "\n",
    "# prepare a uniform distribution to sample for the alpha parameter\n",
    "param_grid = {'alpha': sp_rand()}\n",
    "\n",
    "# create and fit a ridge regression model, testing random alpha values\n",
    "model = Ridge()\n",
    "rsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100)\n",
    "rsearch.fit(dataset.data, dataset.target)\n",
    "print(rsearch)\n",
    "\n",
    "# summarize the results of the random parameter search\n",
    "print(rsearch.best_score_)\n",
    "print(rsearch.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Grid Search vs Random Search: Which one is better?\n",
    "\n",
    "* http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf\n",
    "* https://medium.com/rants-on-machine-learning/smarter-parameter-sweeps-or-why-grid-search-is-plain-stupid-c17d97a0e881#.z27w2p3sr\n",
    "* http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html\n",
    "* Hyperparameter Tuning - http://blog.sigopt.com/post/144221180573/evaluating-hyperparameter-optimization-strategies\n",
    "\n",
    "It has been found that random search performs better than grid search.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/936/1*ZTlQm_WRcrNqL-nLnx6GJA.png)\n",
    "source: http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`DEMO 2.19`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "from operator import itemgetter\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# get some data\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# build a classifier\n",
    "clf = RandomForestClassifier(n_estimators=20)\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")\n",
    "        \n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 11),\n",
    "              \"min_samples_split\": sp_randint(2, 11),\n",
    "              \"min_samples_leaf\": sp_randint(1, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 100\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "start = time()\n",
    "random_search.fit(X, y)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "report(random_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# use a full grid over all parameters\n",
    "param_grid = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid)\n",
    "start = time()\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.grid_scores_)))\n",
    "report(grid_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Summary\n",
    "\n",
    "* The randomized search and the grid search explore exactly the same space of parameters.\n",
    "* The result in parameter settings is quite similar, while the run time for randomized search can be drastically lower.\n",
    "* The performance is slightly worse for the randomized search, though this is most likely a noise effect and would not carry over to a held-out test set.\n",
    "* Note that in practice, one would not search over this many different parameters simultaneously using grid search, but pick only the ones deemed most important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Parameter Estimation with grid search and cross-validation\n",
    "\n",
    "* development set comprises only half of the available labeled data\n",
    "* the performance of the selected hyper-parameters and trained model is then measured on a dedicated evaluation set that was not used during the model selection step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`DEMO 2.20`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Loading the Digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# To apply an classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "X = digits.images.reshape((n_samples, -1))\n",
    "y = digits.target\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5,\n",
    "                       scoring='%s_weighted' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    for params, mean_score, scores in clf.grid_scores_:\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean_score, scores.std() * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
